{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks - bert, coref, ner, nli, paraphrase, qa, sa, srl, ss, sum, wsd\n",
    "task = \"bert\"\n",
    "\n",
    "if task == \"bert\":\n",
    "    from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "elif task == \"coref\":\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"nielsr/coref-bert-base\")\n",
    "    model = AutoModel.from_pretrained(\"nielsr/coref-bert-base\")\n",
    "elif task == \"ner\":\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "elif task == \"nli\":\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"sentence-transformers/bert-base-nli-mean-tokens\"\n",
    "    )\n",
    "    model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "elif task == \"paraphrase\":\n",
    "    from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "    model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
    "elif task == \"qa\":\n",
    "    from transformers import AutoTokenizer, BertForQuestionAnswering\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "    model = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\n",
    "elif task == \"sa\":\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"barissayil/bert-sentiment-analysis-sst\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"barissayil/bert-sentiment-analysis-sst\"\n",
    "    )\n",
    "elif task == \"srl\":\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"liaad/srl-en_mbert-base\")\n",
    "    model = AutoModel.from_pretrained(\"liaad/srl-en_mbert-base\")\n",
    "elif task == \"ss\":\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"vblagoje/bert-english-uncased-finetuned-chunk\"\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        \"vblagoje/bert-english-uncased-finetuned-chunk\"\n",
    "    )\n",
    "elif task == \"sum\":\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lidiya/bart-base-samsum\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"lidiya/bart-base-samsum\")\n",
    "elif task == \"wsd\":\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"./bert-wsd\")\n",
    "    model = AutoModel.from_pretrained(\"./bert-wsd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimuli_384 = open(\n",
    "    \"./pereira_dataset/stimuli_384sentences_dereferencedpronouns.txt\", \"r\"\n",
    ")\n",
    "stimuli_384 = stimuli_384.read()\n",
    "\n",
    "stimuli_243 = open(\n",
    "    \"./pereira_dataset/stimuli_243sentences_dereferencedpronouns.txt\", \"r\"\n",
    ")\n",
    "stimuli_243 = stimuli_243.read()\n",
    "\n",
    "\n",
    "sentences = []\n",
    "sentences.extend(stimuli_384.split(\"\\n\"))\n",
    "sentences.pop()\n",
    "sentences.extend(stimuli_243.split(\"\\n\"))\n",
    "sentences.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    question = sentence\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    if task == \"sum\":\n",
    "        features.append(outputs.encoder_last_hidden_state)\n",
    "    else:\n",
    "        features.append(outputs.hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vectors = []\n",
    "for sentence in features:\n",
    "    temp2 = []\n",
    "    for layers in sentence:\n",
    "        temp1 = []\n",
    "        for tensor_list in layers:\n",
    "            temp_arr = tensor_list.tolist()\n",
    "            up = len(temp_arr) - 1\n",
    "            sliced_arr = temp_arr[1:up]\n",
    "            sliced_with_numpy = []\n",
    "            for word_embedding_list in sliced_arr:\n",
    "                sliced_with_numpy.append(np.array(word_embedding_list))\n",
    "            cnt = 0\n",
    "            s = np.zeros(768)\n",
    "            for i in sliced_with_numpy:\n",
    "                s += i\n",
    "                cnt += 1\n",
    "            sentence_average_arr = s / cnt\n",
    "            tensor_list = sentence_average_arr\n",
    "        temp2.append(tensor_list)\n",
    "    final_vectors.append(temp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = []\n",
    "temp = []\n",
    "for layer in range(len(final_vectors[0])):\n",
    "    temp1 = []\n",
    "    for sentence in range(len(final_vectors)):\n",
    "        temp1.append(final_vectors[sentence][layer])\n",
    "    temp.append(temp1)\n",
    "\n",
    "final_features = np.array(temp)\n",
    "final_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"./features/pereira_{task}.npy\", final_features)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
