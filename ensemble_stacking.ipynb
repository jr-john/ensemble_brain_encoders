{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import spatial\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_accuracy(actual, predicted):\n",
    "    true = 0\n",
    "    total = 0\n",
    "    for i in range(0, len(actual)):\n",
    "        for j in range(i + 1, len(actual)):\n",
    "            total += 1\n",
    "\n",
    "            s1 = actual[i]\n",
    "            s2 = actual[j]\n",
    "            b1 = predicted[i]\n",
    "            b2 = predicted[j]\n",
    "\n",
    "            result1 = spatial.distance.cosine(s1, b1)\n",
    "            result2 = spatial.distance.cosine(s2, b2)\n",
    "            result3 = spatial.distance.cosine(s1, b2)\n",
    "            result4 = spatial.distance.cosine(s2, b1)\n",
    "\n",
    "            if result1 + result2 < result3 + result4:\n",
    "                true += 1\n",
    "\n",
    "    return true / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearcorr(actual, predicted):\n",
    "    corr = []\n",
    "    for i in range(0, len(actual)):\n",
    "        corr.append(np.corrcoef(actual[i], predicted[i])[0][1])\n",
    "    return np.mean(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indices(data):\n",
    "    Taskindices = []\n",
    "    for j in data[\"meta\"][0][0][11][0][5]:\n",
    "        for k in j[0]:\n",
    "            Taskindices.append(int(k))\n",
    "    DMNindices = []\n",
    "    for j in data[\"meta\"][0][0][11][0][6]:\n",
    "        for k in j[0]:\n",
    "            DMNindices.append(int(k))\n",
    "    Visualindices = []\n",
    "    Visualindices_body = []\n",
    "    Visualindices_face = []\n",
    "    Visualindices_object = []\n",
    "    Visualindices_scene = []\n",
    "    for j in data[\"meta\"][0][0][11][0][9]:\n",
    "        for k in j[0]:\n",
    "            Visualindices_body.append(int(k))\n",
    "    for j in data[\"meta\"][0][0][11][0][10]:\n",
    "        for k in j[0]:\n",
    "            Visualindices_face.append(int(k))\n",
    "    for j in data[\"meta\"][0][0][11][0][11]:\n",
    "        for k in j[0]:\n",
    "            Visualindices_object.append(int(k))\n",
    "    for j in data[\"meta\"][0][0][11][0][12]:\n",
    "        for k in j[0]:\n",
    "            Visualindices_scene.append(int(k))\n",
    "\n",
    "    for j in data[\"meta\"][0][0][11][0][13]:\n",
    "        for k in j[0]:\n",
    "            Visualindices.append(int(k))\n",
    "    Languageindices_lh = []\n",
    "    Languageindices_rh = []\n",
    "    for j in data[\"meta\"][0][0][11][0][7]:\n",
    "        for k in j[0]:\n",
    "            Languageindices_lh.append(int(k))\n",
    "    for j in data[\"meta\"][0][0][11][0][8]:\n",
    "        for k in j[0]:\n",
    "            Languageindices_rh.append(int(k))\n",
    "    return (\n",
    "        Taskindices,\n",
    "        DMNindices,\n",
    "        Visualindices_body,\n",
    "        Visualindices_face,\n",
    "        Visualindices_object,\n",
    "        Visualindices_scene,\n",
    "        Visualindices,\n",
    "        Languageindices_lh,\n",
    "        Languageindices_rh,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = [\n",
    "    \"bert\",\n",
    "    \"coref\",\n",
    "    \"ner\",\n",
    "    \"nli\",\n",
    "    \"paraphrase\",\n",
    "    \"qa\",\n",
    "    \"sa\",\n",
    "    \"srl\",\n",
    "    \"ss\",\n",
    "    \"sum\",\n",
    "    \"wsd\",\n",
    "]\n",
    "ROIS = [\n",
    "    \"language_lh\",\n",
    "    \"language_rh\",\n",
    "    \"vision_body\",\n",
    "    \"vision_face\",\n",
    "    \"vision_object\",\n",
    "    \"vision_scene\",\n",
    "    \"vision\",\n",
    "    \"dmn\",\n",
    "    \"task\",\n",
    "]\n",
    "subjects = [\"P01\", \"M02\", \"M04\", \"M07\", \"M15\"]\n",
    "layers_bert = [\n",
    "    \"block1\",\n",
    "    \"block2\",\n",
    "    \"block3\",\n",
    "    \"block4\",\n",
    "    \"block5\",\n",
    "    \"block6\",\n",
    "    \"block7\",\n",
    "    \"block8\",\n",
    "    \"block9\",\n",
    "    \"block10\",\n",
    "    \"block11\",\n",
    "    \"block12\",\n",
    "    \"fc\",\n",
    "]\n",
    "layers_bart = [\"fc\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_data(subject):\n",
    "    data_pic1 = loadmat(\"./pereira_dataset/\" + subject + \"/data_384sentences.mat\")\n",
    "    data_pic2 = loadmat(\"./pereira_dataset/\" + subject + \"/data_243sentences.mat\")\n",
    "    data_pic = loadmat(\"./pereira_dataset/\" + subject + \"/data_384sentences.mat\")\n",
    "\n",
    "    data_pic[\"examples_passagesentences\"] = np.concatenate(\n",
    "        (\n",
    "            data_pic1[\"examples_passagesentences\"],\n",
    "            data_pic2[\"examples_passagesentences\"],\n",
    "        ),\n",
    "        axis=0,\n",
    "    )\n",
    "    data_pic[\"meta\"] = np.concatenate((data_pic1[\"meta\"], data_pic2[\"meta\"]), axis=0)\n",
    "    (\n",
    "        Taskindices,\n",
    "        DMNindices,\n",
    "        Visualindices_body,\n",
    "        Visualindices_face,\n",
    "        Visualindices_object,\n",
    "        Visualindices_scene,\n",
    "        Visualindices,\n",
    "        Languageindices_lh,\n",
    "        Languageindices_rh,\n",
    "    ) = generate_indices(data_pic)\n",
    "\n",
    "    roi_indices = {\n",
    "        \"language_lh\": Languageindices_lh,\n",
    "        \"language_rh\": Languageindices_rh,\n",
    "        \"vision_body\": Visualindices_body,\n",
    "        \"vision_face\": Visualindices_face,\n",
    "        \"vision_object\": Visualindices_object,\n",
    "        \"vision_scene\": Visualindices_scene,\n",
    "        \"vision\": Visualindices,\n",
    "        \"dmn\": DMNindices,\n",
    "        \"task\": Taskindices,\n",
    "    }\n",
    "\n",
    "    fmri = {}\n",
    "    for roi, indices in roi_indices.items():\n",
    "        fmri[roi] = data_pic[\"examples_passagesentences\"][0:, np.array(indices) - 1]\n",
    "\n",
    "    return fmri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ensemble(outputs, voxels, typ=\"lin_reg\"):\n",
    "    voxels = np.array(voxels)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        outputs, voxels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    if typ == \"lin_reg\":\n",
    "        reg = LinearRegression()\n",
    "        for x_sentence, y_sentence in zip(x_train, y_train):\n",
    "            reg.fit(x_sentence, y_sentence)\n",
    "        y_pred = list()\n",
    "        for x_sentence in x_test:\n",
    "            y_pred.append(reg.predict(x_sentence))\n",
    "        y_pred = np.array(y_pred)\n",
    "    else:\n",
    "        pass\n",
    "    acc = pairwise_accuracy(y_test, y_pred)\n",
    "    corr = pearcorr(y_test, y_pred)\n",
    "\n",
    "    return acc, corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = {}\n",
    "output = {}\n",
    "output[\"2v2\"] = {}\n",
    "output[\"pear\"] = {}\n",
    "\n",
    "for subject in tqdm(subjects[:1]):\n",
    "    ensemble[subject] = {}\n",
    "    output[\"2v2\"][subject] = {}\n",
    "    output[\"pear\"][subject] = {}\n",
    "    fmri = get_subject_data(subject)\n",
    "    for roi in ROIS[:1]:\n",
    "        model_outputs = []\n",
    "        for task in TASKS[:3]:\n",
    "            print(f\"{subject} {roi} {task}\")\n",
    "            data = np.load(f\"./features/pereira_{task}.npy\", allow_pickle=True)\n",
    "            feats = np.array(data[-1])\n",
    "            with open(f\"./models/{task}_{roi}_{subject}.model\", \"rb\") as f:\n",
    "                model = pickle.load(f)\n",
    "\n",
    "            model_output = model.predict(feats)\n",
    "            model_outputs.append(model_output)\n",
    "            del model, data\n",
    "\n",
    "        model_outputs = np.array(model_outputs)\n",
    "        model_outputs = np.transpose(model_outputs, (1, 0, 2))\n",
    "        voxels = np.array(fmri[roi])\n",
    "        acc, corr = create_ensemble(model_outputs, voxels)\n",
    "        output[\"2v2\"][subject][roi] = acc\n",
    "        output[\"pear\"][subject][roi] = corr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f\"results/results_ensemble.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(output, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
